# Hadoop

## Hadoop Configuration

**core-site.xml**: This file is a part of the Hadoop configuration files, which are used to inform the Hadoop daemon where the NameNode (the centerpiece of an HDFS file system) runs in the cluster³⁴. It contains settings for Hadoop Core, such as I/O settings, that are common to HDFS (Hadoop Distributed File System) and MapReduce³⁴. These settings are crucial for the proper functioning of a Hadoop cluster³⁴.

**NameNode**: In Hadoop, the NameNode is the master node that manages the file system metadata. Each cluster has a single NameNode, and if that machine crashes, the entire Hadoop system goes down³⁴.

**Hadoop Core**: This refers to the basic components of the Hadoop system, including the Hadoop Common (libraries and utilities), HDFS, and MapReduce³⁴.

**HDFS and MapReduce**: HDFS is the primary storage system used by Hadoop applications, while MapReduce is a computational model that allows for processing large data sets in parallel³⁴.

**hdfs-site.xml**: This is a configuration file for Hadoop, specifically for the Hadoop Distributed File System (HDFS) daemons¹³. Daemons are background services running on a system.

**NameNode, Secondary NameNode, and DataNodes**: These are the main components of HDFS¹³. The NameNode is the master server that manages the file system namespace and regulates access to files by clients. The Secondary NameNode works with the NameNode to help ensure the reliability of the data. DataNodes are the workhorses of HDFS, storing and retrieving data blocks when they are told to (by clients or the NameNode), and reporting back to the NameNode periodically with lists of blocks that they are storing¹³.

**Default block replication and permission checking on HDFS**: These are some of the parameters that can be configured in the hdfs-site.xml file¹³. Block replication refers to the number of copies of data blocks. Permission checking refers to the access control on HDFS¹³.

**Replication specified at create time**: When a file is created in HDFS, you can specify the number of replications for that file. If not specified, the default value from the hdfs-site.xml file is used¹³.

**The most critical properties in core-site.xml file**  
-   `fs.defaultFS`: This property specifies the default file system name and the location of the NameNode. For example, `hdfs://localhost:9000` specifies the local machine and port number used by the NameNode¹.  
-   `hadoop.tmp.dir`: This property specifies the directory path for storing temporary files generated by Hadoop. For example, `/tmp/hadoop-${user.name}` specifies the `/tmp` directory and the user name¹.

**Here is a typocal use case of a Hadoop cluster**  
One "master" server/Node, and you specify on it "NameNode: http://master:50070", "JobTracker: http://master:50030"  
Another "secondary NameNode"  
Finally one or more "slave" server/Node, and you need to sepecify the "DataNode: http...", and "TaskTracker: http://..." for each one of them  

A **Single Node Cluster** in Hadoop is a type of setup where all the Hadoop daemons, including the NameNode, DataNode, Secondary NameNode, ResourceManager, and NodeManager, run on a single machine¹²³. This setup is often used for learning, experimenting, and testing purposes¹²³.

In this context, the terms "master" and "slave" refer to the roles that different nodes play in the Hadoop architecture¹²³. In a Single Node Cluster, both the master and slave services are running on the same machine¹²³.

Here's a brief explanation of these roles:

- **Master Nodes**: These are the nodes that control and manage the data storage and processing tasks in the cluster¹²³. In Hadoop, the NameNode and ResourceManager are considered master nodes¹²³.
    - **NameNode**: Manages the file system metadata and regulates access to files by clients¹²³.
    - **ResourceManager**: Manages the use of resources across the cluster¹²³.

- **Slave Nodes**: These are the nodes that store the data and perform the computations¹²³. In Hadoop, the DataNode and NodeManager are considered slave nodes¹²³.
    - **DataNode**: Stores and retrieves data blocks when they are told to (by clients or the NameNode), and reports back to the NameNode periodically with lists of blocks that they are storing¹²³.
    - **NodeManager**: Manages the execution of tasks on each data node¹²³.

So, in a Single Node Cluster, the same machine acts as both the master and the slave, running all the services¹²³.

